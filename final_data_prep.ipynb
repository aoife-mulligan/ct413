{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This data preparation will focus on the features used in Toward a Holistic Computational Representation for Sleep Quality and its Support for Explainability\n",
    "\n",
    "The data will be prepared for Linear Regression, Random Forest Regressor, and Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "folder_path = 'c:/Users/aoife/Documents/Project/DataTables/'\n",
    "save_path = 'C:/Users/aoife/Documents/Project/second-approach/'\n",
    "\n",
    "df_onboarding = pd.read_csv(folder_path + 'onboarding-demographics.csv', usecols=['participantId', 'age_years', 'gender', 'timestamp'])\n",
    "df_bm = pd.read_csv(folder_path + 'body-mass.csv', usecols=['participantId', 'value', 'timestamp'])\n",
    "df_height = pd.read_csv(folder_path + 'height.csv', usecols=['participantId', 'value', 'timestamp'])\n",
    "df_my_health = pd.read_csv(folder_path + 'my-health.csv', usecols=['participantId', 'stressed', 'cancer', 'diabetes', 'timestamp'])\n",
    "df_hr = pd.read_csv(folder_path + 'heart-rate.csv', usecols=['participantId', 'value', 'startTime', 'endTime', 'timestamp'])\n",
    "df_nt = pd.read_csv(folder_path + 'nap-tracker.csv', usecols=['participantId', 'NapDuration', 'timestamp'])\n",
    "df_about_me = pd.read_csv(folder_path + 'about-me.csv', usecols=['participantId', 'menopause', 'timestamp'])\n",
    "df_steps = pd.read_csv(folder_path + 'step-count.csv', usecols=['participantId', 'value', 'startTime', 'endTime', 'timestamp'])\n",
    "df_sq = pd.read_csv(folder_path + 'sleep-quality-checker.csv', usecols=['participantId', 'value', 'timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timestamp data to datetime in all dataframes\n",
    "\n",
    "df_onboarding['timestamp'] = pd.to_datetime(df_onboarding['timestamp'], utc=True)\n",
    "df_my_health['timestamp'] = pd.to_datetime(df_my_health['timestamp'], utc=True)\n",
    "df_hr['timestamp'] = pd.to_datetime(df_hr['timestamp'], utc=True)\n",
    "df_hr['startTime'] = pd.to_datetime(df_hr['startTime'], utc=True)\n",
    "df_hr['endTime'] = pd.to_datetime(df_hr['endTime'], utc=True)\n",
    "df_nt['timestamp'] = pd.to_datetime(df_nt['timestamp'], utc=True)\n",
    "df_about_me['timestamp'] = pd.to_datetime(df_about_me['timestamp'], utc=True)\n",
    "df_steps['timestamp'] = pd.to_datetime(df_steps['timestamp'], utc=True)\n",
    "df_steps['startTime'] = pd.to_datetime(df_steps['startTime'], utc=True)\n",
    "df_steps['endTime'] = pd.to_datetime(df_steps['endTime'], utc=True)\n",
    "df_sq['timestamp'] = pd.to_datetime(df_sq['timestamp'], utc=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          participantId  gender  age_years  \\\n",
      "0  3d6c0442-9150-4974-8257-3a94461c8790    Male       27.0   \n",
      "1  a12e2ca0-2cf4-498f-a51d-1644630511ec  Female       19.0   \n",
      "2  f63a6a72-88c1-4b49-9326-578f33ed8e9a    Male       51.0   \n",
      "3  00a347ef-8bac-4b44-8820-338fd910d4e0    Male       34.0   \n",
      "4  f3f37736-a61f-414e-903a-9de0d92d1ded    Male       19.0   \n",
      "\n",
      "                  timestamp   startDate  \n",
      "0 2016-07-21 21:44:06+00:00  2016-07-21  \n",
      "1 2016-07-22 00:29:50+00:00  2016-07-22  \n",
      "2 2016-03-10 16:34:55+00:00  2016-03-10  \n",
      "3 2016-03-04 16:15:23+00:00  2016-03-04  \n",
      "4 2016-04-02 17:50:39+00:00  2016-04-02  \n",
      "                          participantId             timestamp    value\n",
      "0  11b2bf4d-6020-4a86-81bd-237c5616c649  2016-08-24T23:56:14Z   97.976\n",
      "1  9cbbb597-30c0-4f48-aebd-7fe58c627cf6  2016-03-16T02:33:20Z   63.503\n",
      "2  941e9fa4-6ba5-41f5-9bcc-805849d751b8  2017-01-01T14:07:00Z   96.717\n",
      "3  41c683b0-f6c4-4a6a-8dd2-4b9a3dd30a32  2016-03-30T02:24:59Z  113.398\n",
      "4  35fbae45-0f21-45ea-8ed7-8d3361a29b86  2016-07-18T17:45:38Z   72.121\n",
      "                          participantId             timestamp   value\n",
      "0  11b2bf4d-6020-4a86-81bd-237c5616c649  2016-08-24T23:56:14Z  1.6256\n",
      "1  9cbbb597-30c0-4f48-aebd-7fe58c627cf6  2016-03-16T02:33:20Z  1.6510\n",
      "2  941e9fa4-6ba5-41f5-9bcc-805849d751b8  2017-01-01T14:07:00Z  1.8288\n",
      "3  1cba6631-3fb8-4c18-904f-ab56fe82833d  2016-03-07T05:54:00Z  1.5494\n",
      "4  eb2c4ff0-54e2-4c34-b73d-003e3e0427ad  2016-04-02T07:17:47Z  1.7018\n",
      "                          participantId  cancer  diabetes  stressed  \\\n",
      "0  f2514967-9173-4834-96f7-0acdd0298e84       2         2         3   \n",
      "1  7a8e66eb-7c5d-4e55-8967-f3f46f781253       2         1         5   \n",
      "2  2b8a2d5f-f9b1-416d-84f5-a2d87384cc56       2         2         4   \n",
      "3  11599500-9817-47ff-b036-019d6fa85bbf       2         2         3   \n",
      "4  0eba3ad1-3fd1-46d3-9771-dfff5c477b96       2         2         1   \n",
      "\n",
      "                  timestamp   startDate  \n",
      "0 2016-03-03 15:28:00+00:00  2016-03-03  \n",
      "1 2016-09-18 04:05:39+00:00  2016-09-18  \n",
      "2 2016-09-18 14:28:16+00:00  2016-09-18  \n",
      "3 2016-03-04 07:25:44+00:00  2016-03-04  \n",
      "4 2016-03-24 08:39:55+00:00  2016-03-24  \n",
      "                          participantId                 timestamp  \\\n",
      "0  06bc6ebb-a233-469f-8091-90256f656b1b 2016-03-13 19:25:18+00:00   \n",
      "1  06bc6ebb-a233-469f-8091-90256f656b1b 2016-03-13 19:25:18+00:00   \n",
      "2  06bc6ebb-a233-469f-8091-90256f656b1b 2016-03-13 19:25:18+00:00   \n",
      "3  06bc6ebb-a233-469f-8091-90256f656b1b 2016-03-13 19:25:18+00:00   \n",
      "4  06bc6ebb-a233-469f-8091-90256f656b1b 2016-03-13 19:25:18+00:00   \n",
      "\n",
      "                  startTime                   endTime  value   startDate  \n",
      "0 2016-03-13 13:52:58+00:00 2016-03-13 13:52:58+00:00  1.500  2016-03-13  \n",
      "1 2016-03-13 13:53:00+00:00 2016-03-13 13:53:00+00:00  1.500  2016-03-13  \n",
      "2 2016-03-13 13:53:01+00:00 2016-03-13 13:53:01+00:00  1.517  2016-03-13  \n",
      "3 2016-03-13 13:53:02+00:00 2016-03-13 13:53:02+00:00  1.533  2016-03-13  \n",
      "4 2016-03-13 13:53:03+00:00 2016-03-13 13:53:03+00:00  1.550  2016-03-13  \n",
      "                          participantId  NapDuration  \\\n",
      "0  cb4c4df0-6685-4743-a3cf-d29c3d6941c9       4500.0   \n",
      "1  1f230684-4fa7-48ce-a7c2-6372238fe486      24708.0   \n",
      "2  02b96d81-c9f6-4a73-a075-75a64e54d005       3300.0   \n",
      "3  28e7a976-55e2-4ff1-8e95-7278f1ee9ec9          0.0   \n",
      "4  1dcf514b-8610-40f3-9a78-43ef80b5b7b7       4093.0   \n",
      "\n",
      "                  timestamp   startDate  \n",
      "0 2016-07-11 02:37:13+00:00  2016-07-11  \n",
      "1 2016-09-07 12:20:41+00:00  2016-09-07  \n",
      "2 2016-03-12 01:47:15+00:00  2016-03-12  \n",
      "3 2016-09-07 22:48:16+00:00  2016-09-07  \n",
      "4 2016-09-08 13:21:58+00:00  2016-09-08  \n",
      "                          participantId  menopause                 timestamp  \\\n",
      "0  0c82c9d1-25ba-4cb2-95df-f79fca0b8464        3.0 2016-03-04 22:54:18+00:00   \n",
      "1  20a71d11-3d78-4ee0-a172-5dd8f7e33bd2        NaN 2016-03-04 22:59:09+00:00   \n",
      "2  f5aac809-fd16-4733-83ee-0991eaf7036f        NaN 2016-03-04 23:11:14+00:00   \n",
      "3  5ceb42a9-cd99-4ba8-93d8-4fc4a5de5a4f        NaN 2016-03-04 23:22:08+00:00   \n",
      "4  80bfb0f6-601c-47ed-9538-bedb8eb6c69f        3.0 2016-03-04 23:40:37+00:00   \n",
      "\n",
      "    startDate  \n",
      "0  2016-03-04  \n",
      "1  2016-03-04  \n",
      "2  2016-03-04  \n",
      "3  2016-03-04  \n",
      "4  2016-03-04  \n",
      "                          participantId                 timestamp  \\\n",
      "0  1f649060-680a-4c80-a551-be38ce46cb94 2016-05-14 03:04:03+00:00   \n",
      "1  1f649060-680a-4c80-a551-be38ce46cb94 2016-05-14 03:04:03+00:00   \n",
      "2  1f649060-680a-4c80-a551-be38ce46cb94 2016-05-14 03:04:03+00:00   \n",
      "3  1f649060-680a-4c80-a551-be38ce46cb94 2016-05-14 03:04:03+00:00   \n",
      "4  1f649060-680a-4c80-a551-be38ce46cb94 2016-05-14 03:04:03+00:00   \n",
      "\n",
      "                  startTime                   endTime  value   startDate  \n",
      "0 2016-03-22 22:07:20+00:00 2016-03-22 22:08:20+00:00    118  2016-03-22  \n",
      "1 2016-03-22 22:08:20+00:00 2016-03-22 22:09:20+00:00    102  2016-03-22  \n",
      "2 2016-03-22 22:09:20+00:00 2016-03-22 22:10:20+00:00     70  2016-03-22  \n",
      "3 2016-03-22 22:10:20+00:00 2016-03-22 22:11:21+00:00    106  2016-03-22  \n",
      "4 2016-03-22 22:11:21+00:00 2016-03-22 22:12:21+00:00    103  2016-03-22  \n",
      "                          participantId  value                 timestamp  \\\n",
      "0  b4ebf7dd-4e30-4f7b-8ee8-5493a19c8c9f      4 2016-09-27 02:18:24+00:00   \n",
      "1  c3993552-69cb-45e4-b18a-5e6eecefb825      4 2016-03-07 14:16:09+00:00   \n",
      "2  78f60bd3-34f3-489e-a352-f9df564641c3      4 2016-03-05 22:21:46+00:00   \n",
      "3  9da1a89a-2145-4cca-b356-7b58aa7be8b0      4 2016-09-27 02:41:55+00:00   \n",
      "4  4aad9dbe-dd9e-4832-a198-3bd563457124      4 2016-03-03 18:52:42+00:00   \n",
      "\n",
      "    startDate  \n",
      "0  2016-09-27  \n",
      "1  2016-03-07  \n",
      "2  2016-03-05  \n",
      "3  2016-09-27  \n",
      "4  2016-03-03  \n"
     ]
    }
   ],
   "source": [
    "df_hr['startDate'] = df_hr['startTime'].dt.date\n",
    "df_steps['startDate'] = df_steps['startTime'].dt.date\n",
    "\n",
    "# change all timestamps to date only\n",
    "df_onboarding['startDate'] = df_onboarding['timestamp'].dt.date\n",
    "df_my_health['startDate'] = df_my_health['timestamp'].dt.date\n",
    "df_nt['startDate'] = df_nt['timestamp'].dt.date\n",
    "df_about_me['startDate'] = df_about_me['timestamp'].dt.date\n",
    "df_sq['startDate'] = df_sq['timestamp'].dt.date\n",
    "\n",
    "\n",
    "# preview\n",
    "print(df_onboarding.head())\n",
    "print(df_bm.head())\n",
    "print(df_height.head())\n",
    "print(df_my_health.head())\n",
    "print(df_hr.head())\n",
    "print(df_nt.head())\n",
    "print(df_about_me.head())\n",
    "print(df_steps.head())\n",
    "print(df_sq.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          participantId   startDate  morning_hr  afternoon_hr  \\\n",
      "0  06bc6ebb-a233-469f-8091-90256f656b1b  2016-03-13    1.173082      1.500000   \n",
      "1  2214f4fd-1ae0-4804-8663-b01c5f6d142c  2016-03-02    1.260992      1.333000   \n",
      "2  b2571643-4aec-492f-bc7f-6f23c7fe239a  2018-10-03    1.300000      1.263071   \n",
      "3  b1406c4e-e6ac-4297-a9e4-335ca5ef04de  2019-01-24    1.203742      1.203742   \n",
      "4  b1406c4e-e6ac-4297-a9e4-335ca5ef04de  2019-01-23    1.903400      1.903400   \n",
      "\n",
      "   evening_hr  night_hr  mean_hr_morning  max_hr_morning  min_hr_morning  \\\n",
      "0    1.173082  1.173082         1.159673        1.233000        0.867000   \n",
      "1    1.260992  1.260992         1.260992        1.260992        1.260992   \n",
      "2    1.263071  1.263071         1.285036        1.533000        1.133000   \n",
      "3    1.203742  1.333000         1.203742        1.203742        1.203742   \n",
      "4    1.917000  1.903400         1.903400        1.903400        1.903400   \n",
      "\n",
      "   mean_hr_afternoon  max_hr_afternoon  min_hr_afternoon  mean_hr_evening  \\\n",
      "0           1.185244          2.133000          0.850000         1.174329   \n",
      "1           1.261396          1.783000          1.050000         1.260587   \n",
      "2           1.241107          1.263071          1.183000         1.263071   \n",
      "3           1.203742          1.203742          1.203742         1.193823   \n",
      "4           1.903400          1.903400          1.903400         1.903400   \n",
      "\n",
      "   max_hr_evening  min_hr_evening  mean_hr_night  max_hr_night  min_hr_night  \n",
      "0        1.617000        1.083000       1.173082      1.173082      1.173082  \n",
      "1        1.417000        1.150000       1.260992      1.260992      1.260992  \n",
      "2        1.263071        1.263071       1.263071      1.263071      1.263071  \n",
      "3        1.203742        1.000000       1.213661      1.400000      0.983000  \n",
      "4        1.917000        1.883000       1.903400      1.903400      1.903400  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import time\n",
    "import numpy as np\n",
    "\n",
    "df_hr = pd.read_csv(folder_path + 'heart-rate.csv', usecols=['participantId', 'timestamp', 'startTime', 'endTime', 'value'], parse_dates=['timestamp', 'startTime', 'endTime'])\n",
    "\n",
    "# Convert startTime and endTime to datetime and get date\n",
    "df_hr['startTime'] = pd.to_datetime(df_hr['startTime'])\n",
    "df_hr['endTime'] = pd.to_datetime(df_hr['endTime'])\n",
    "df_hr['startDate'] = df_hr['startTime'].dt.date\n",
    "\n",
    "# Remove rows with duplicated participantId, startTime, and endTime\n",
    "df_hr.drop_duplicates(subset=['participantId', 'startTime', 'endTime'], inplace=True)\n",
    "\n",
    "# Initialize time of day columns as NaN\n",
    "df_hr['morning_hr'] = np.nan\n",
    "df_hr['afternoon_hr'] = np.nan\n",
    "df_hr['evening_hr'] = np.nan\n",
    "df_hr['night_hr'] = np.nan\n",
    "\n",
    "# Define time boundaries\n",
    "# TODO look into what times are best for these\n",
    "morning_end = time(12, 0)\n",
    "afternoon_end = time(18, 0)\n",
    "evening_end = time(23, 59)\n",
    "night_end = time(6, 0)\n",
    "\n",
    "# Classify each heart rate entry into the correct time of day\n",
    "for index, row in df_hr.iterrows():\n",
    "    start_time = row['startTime'].time()\n",
    "    if night_end <= start_time < morning_end:\n",
    "        df_hr.at[index, 'morning_hr'] = row['value']\n",
    "    elif morning_end <= start_time < afternoon_end:\n",
    "        df_hr.at[index, 'afternoon_hr'] = row['value']\n",
    "    elif afternoon_end <= start_time < evening_end:\n",
    "        df_hr.at[index, 'evening_hr'] = row['value']\n",
    "    else:\n",
    "        df_hr.at[index, 'night_hr'] = row['value']\n",
    "\n",
    "# Calculate a temporary daily average heart rate excluding NaN values for accurate imputation\n",
    "df_hr['temp_daily_avg_hr'] = df_hr[['morning_hr', 'afternoon_hr', 'evening_hr', 'night_hr']].mean(axis=1, skipna=True)\n",
    "\n",
    "# Group by participant and date to compute an accurate daily average for imputation, excluding days with all NaN values\n",
    "accurate_daily_avg = df_hr.groupby(['participantId', 'startDate'])['temp_daily_avg_hr'].mean().reset_index().rename(columns={'temp_daily_avg_hr': 'accurate_daily_avg_hr'})\n",
    "\n",
    "# Merge this accurate daily average back into the original dataframe\n",
    "df_hr = pd.merge(df_hr, accurate_daily_avg, on=['participantId', 'startDate'], how='left')\n",
    "\n",
    "# Impute missing values in each period with the recalculated accurate daily average\n",
    "for period in ['morning_hr', 'afternoon_hr', 'evening_hr', 'night_hr']:\n",
    "    df_hr[period] = df_hr[period].fillna(df_hr['accurate_daily_avg_hr'])\n",
    "\n",
    "# Calculate mean, max, and min heart rate for each period (includes imputed values)\n",
    "df_hr_aggregated = df_hr.groupby(['participantId', 'startDate']).agg(\n",
    "    mean_hr_morning=('morning_hr', 'mean'),\n",
    "    max_hr_morning=('morning_hr', 'max'),\n",
    "    min_hr_morning=('morning_hr', 'min'),\n",
    "    mean_hr_afternoon=('afternoon_hr', 'mean'),\n",
    "    max_hr_afternoon=('afternoon_hr', 'max'),\n",
    "    min_hr_afternoon=('afternoon_hr', 'min'),\n",
    "    mean_hr_evening=('evening_hr', 'mean'),\n",
    "    max_hr_evening=('evening_hr', 'max'),\n",
    "    min_hr_evening=('evening_hr', 'min'),\n",
    "    mean_hr_night=('night_hr', 'mean'),\n",
    "    max_hr_night=('night_hr', 'max'),\n",
    "    min_hr_night=('night_hr', 'min')\n",
    ").reset_index()\n",
    "\n",
    "# Drop the temporary columns\n",
    "df_hr_summary = df_hr[['participantId', 'startDate', 'morning_hr', 'afternoon_hr', 'evening_hr', 'night_hr']].drop_duplicates(subset=['participantId', 'startDate'])\n",
    "\n",
    "# Merge the aggregated values\n",
    "df_hr_final = pd.merge(df_hr_summary, df_hr_aggregated, on=['participantId', 'startDate'], how='left')\n",
    "\n",
    "df_hr_final.drop_duplicates(subset=['participantId', 'startDate'], keep='first', inplace=True)\n",
    "\n",
    "# savr to csv just to check all the entries\n",
    "df_hr_final.to_csv(save_path + 'heart_rate_summary.csv', index=False)\n",
    "\n",
    "print(df_hr_final.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            participantId                 timestamp  \\\n",
      "0    1f649060-680a-4c80-a551-be38ce46cb94 2016-05-14 03:04:03+00:00   \n",
      "21   1f649060-680a-4c80-a551-be38ce46cb94 2016-05-14 03:04:03+00:00   \n",
      "91   1f649060-680a-4c80-a551-be38ce46cb94 2016-05-14 03:04:03+00:00   \n",
      "116  1f649060-680a-4c80-a551-be38ce46cb94 2016-05-14 03:04:07+00:00   \n",
      "127  1f649060-680a-4c80-a551-be38ce46cb94 2016-05-14 03:04:07+00:00   \n",
      "\n",
      "      startDate  steps_per_hour  morningStepsTotal  afternoonStepsTotal  \\\n",
      "0    2016-03-22     3296.571899                0.0               4422.0   \n",
      "21   2016-03-23     3838.334779                0.0               8885.0   \n",
      "91   2016-03-24     4264.419532               20.0               7859.0   \n",
      "116  2016-03-29     3292.721759              118.0               8130.0   \n",
      "127  2016-03-30     3490.214189                0.0               6803.0   \n",
      "\n",
      "     eveningStepsTotal  nightStepsTotal  totalSteps  stepsTotalDuration  \n",
      "0               5529.0             66.0       10017             10939.0  \n",
      "21              5568.0            299.0       14752             13836.0  \n",
      "91              5569.0           2296.0       15744             13291.0  \n",
      "116             5256.0            973.0       14477             15828.0  \n",
      "127             4224.0           1149.0       12176             12559.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import time, datetime\n",
    "import numpy as np\n",
    "\n",
    "df_sc = pd.read_csv(folder_path + 'step-count.csv', usecols=['participantId', 'timestamp', 'startTime', 'endTime', 'value'], parse_dates=['timestamp', 'startTime', 'endTime'])\n",
    "\n",
    "df_sc['startTime'] = pd.to_datetime(df_sc['startTime'], utc=True)\n",
    "df_sc['endTime'] = pd.to_datetime(df_sc['endTime'], utc=True)\n",
    "df_sc['startDate'] = df_sc['startTime'].dt.date\n",
    "\n",
    "df_sc['duration'] = (df_sc['endTime'] - df_sc['startTime']).dt.total_seconds()\n",
    "\n",
    "\n",
    "# Calculate total steps per day for each participant\n",
    "df_sc['totalSteps'] = df_sc.groupby(['participantId', 'startDate'])['value'].transform('sum')\n",
    "\n",
    "# Calculate the total duration per day for each participant\n",
    "df_sc['stepsTotalDuration'] = df_sc.groupby(['participantId', 'startDate'])['duration'].transform('sum')\n",
    "df_sc['steps_per_hour'] = df_sc['totalSteps'] / (df_sc['stepsTotalDuration'] / (60*60))\n",
    "\n",
    "# Time boundaries\n",
    "morning_end = time(12, 0)\n",
    "afternoon_end = time(18, 0)\n",
    "evening_end = time(23, 59)\n",
    "night_end = time(6, 0)\n",
    "\n",
    "# Initialize step counts for different times of day as NaN\n",
    "df_sc['morningSteps'] = np.nan\n",
    "df_sc['afternoonSteps'] = np.nan\n",
    "df_sc['eveningSteps'] = np.nan\n",
    "df_sc['nightSteps'] = np.nan\n",
    "\n",
    "# Classify steps into different times of the day\n",
    "for index, row in df_sc.iterrows():\n",
    "    start_time = row['startTime'].time()\n",
    "    if night_end < start_time <= morning_end:\n",
    "        df_sc.at[index, 'morningSteps'] = row['value']\n",
    "    elif morning_end < start_time <= afternoon_end:\n",
    "        df_sc.at[index, 'afternoonSteps'] = row['value']\n",
    "    elif afternoon_end < start_time <= evening_end:\n",
    "        df_sc.at[index, 'eveningSteps'] = row['value']\n",
    "    else:\n",
    "        df_sc.at[index, 'nightSteps'] = row['value']\n",
    "\n",
    "# aggregate steps into totals for each time\n",
    "df_summary = df_sc.groupby(['participantId', 'startDate']).agg(\n",
    "    morningStepsTotal=('morningSteps', 'sum'),\n",
    "    afternoonStepsTotal=('afternoonSteps', 'sum'),\n",
    "    eveningStepsTotal=('eveningSteps', 'sum'),\n",
    "    nightStepsTotal=('nightSteps', 'sum'),\n",
    "    totalSteps=('totalSteps', 'first'), \n",
    "    stepsTotalDuration=('stepsTotalDuration', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# drop unnecessary columns\n",
    "df_sc.drop(columns=['morningSteps', 'afternoonSteps', 'eveningSteps', 'nightSteps', 'duration', 'value', 'startTime', 'endTime', 'totalSteps', 'stepsTotalDuration'], inplace=True)\n",
    "# merge back together\n",
    "df_sc = pd.merge(df_sc, df_summary, on=['participantId', 'startDate'], how='left')\n",
    "\n",
    "# Drop duplicates\n",
    "df_sc.drop_duplicates(subset=['participantId', 'startDate'], inplace=True)\n",
    "\n",
    "df_sc.to_csv(save_path + 'step_count_summary.csv', index=False)\n",
    "\n",
    "print(df_sc.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate bmi based on height and weight\n",
    "\n",
    "# rename value columns\n",
    "df_bm.rename(columns={'value': 'weight'}, inplace=True)\n",
    "df_height.rename(columns={'value': 'height'}, inplace=True)\n",
    "\n",
    "# merge height and weight data\n",
    "df_bmi = pd.merge(df_bm, df_height, on=['participantId', 'timestamp'], how='outer')\n",
    "\n",
    "# calculate bmi\n",
    "df_bmi['bmi'] = df_bmi['weight'] / ((df_bmi['height'] / 100) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove startdate\n",
    "\n",
    "df_about_me.drop(columns=['startDate'], inplace=True)\n",
    "df_my_health.drop(columns=['startDate'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for NapDuration in df_nt, fill NaN with 0\n",
    "df_nt['NapDuration'] = df_nt['NapDuration'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          participantId  NapDuration  \\\n",
      "0  cb4c4df0-6685-4743-a3cf-d29c3d6941c9       4500.0   \n",
      "1  1f230684-4fa7-48ce-a7c2-6372238fe486      24708.0   \n",
      "2  02b96d81-c9f6-4a73-a075-75a64e54d005       3300.0   \n",
      "3  28e7a976-55e2-4ff1-8e95-7278f1ee9ec9          0.0   \n",
      "4  1dcf514b-8610-40f3-9a78-43ef80b5b7b7       4093.0   \n",
      "\n",
      "                  timestamp   startDate  \n",
      "0 2016-07-11 02:37:13+00:00  2016-07-11  \n",
      "1 2016-09-07 12:20:41+00:00  2016-09-07  \n",
      "2 2016-03-12 01:47:15+00:00  2016-03-12  \n",
      "3 2016-09-07 22:48:16+00:00  2016-09-07  \n",
      "4 2016-09-08 13:21:58+00:00  2016-09-08  \n",
      "                          participantId  menopause                 timestamp\n",
      "0  0c82c9d1-25ba-4cb2-95df-f79fca0b8464        3.0 2016-03-04 22:54:18+00:00\n",
      "1  20a71d11-3d78-4ee0-a172-5dd8f7e33bd2        NaN 2016-03-04 22:59:09+00:00\n",
      "2  f5aac809-fd16-4733-83ee-0991eaf7036f        NaN 2016-03-04 23:11:14+00:00\n",
      "3  5ceb42a9-cd99-4ba8-93d8-4fc4a5de5a4f        NaN 2016-03-04 23:22:08+00:00\n",
      "4  80bfb0f6-601c-47ed-9538-bedb8eb6c69f        3.0 2016-03-04 23:40:37+00:00\n"
     ]
    }
   ],
   "source": [
    "print(df_nt.head())\n",
    "print(df_about_me.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename sleep quality value column\n",
    "df_sq.rename(columns={'value': 'sq_score'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          participantId   startDate  morning_hr  afternoon_hr  \\\n",
      "0  06bc6ebb-a233-469f-8091-90256f656b1b  2016-03-13    1.173082      1.500000   \n",
      "1  2214f4fd-1ae0-4804-8663-b01c5f6d142c  2016-03-02    1.260992      1.333000   \n",
      "2  b2571643-4aec-492f-bc7f-6f23c7fe239a  2018-10-03    1.300000      1.263071   \n",
      "3  b1406c4e-e6ac-4297-a9e4-335ca5ef04de  2019-01-24    1.203742      1.203742   \n",
      "4  b1406c4e-e6ac-4297-a9e4-335ca5ef04de  2019-01-23    1.903400      1.903400   \n",
      "\n",
      "   evening_hr  night_hr  mean_hr_morning  max_hr_morning  min_hr_morning  \\\n",
      "0    1.173082  1.173082         1.159673        1.233000        0.867000   \n",
      "1    1.260992  1.260992         1.260992        1.260992        1.260992   \n",
      "2    1.263071  1.263071         1.285036        1.533000        1.133000   \n",
      "3    1.203742  1.333000         1.203742        1.203742        1.203742   \n",
      "4    1.917000  1.903400         1.903400        1.903400        1.903400   \n",
      "\n",
      "   mean_hr_afternoon  ...               timestamp_x  steps_per_hour  \\\n",
      "0           1.185244  ... 2016-03-13 19:25:18+00:00     1976.975731   \n",
      "1           1.261396  ... 2016-03-02 19:58:02+00:00     1806.812227   \n",
      "2           1.241107  ... 2018-10-03 12:45:23+00:00     1147.144240   \n",
      "3           1.203742  ... 2019-01-24 03:34:00+00:00      616.763310   \n",
      "4           1.903400  ... 2019-01-23 12:51:24+00:00      519.326657   \n",
      "\n",
      "   morningStepsTotal  afternoonStepsTotal  eveningStepsTotal  nightStepsTotal  \\\n",
      "0               32.0               8646.0              147.0              0.0   \n",
      "1                0.0               1047.0              677.0              0.0   \n",
      "2             1966.0                  9.0                0.0              0.0   \n",
      "3                0.0                  0.0              403.0           1222.0   \n",
      "4              251.0                  0.0              467.0            636.0   \n",
      "\n",
      "   totalSteps  stepsTotalDuration NapDuration  timestamp_y  \n",
      "0      8825.0             16070.0         NaN          NaT  \n",
      "1      1724.0              3435.0         NaN          NaT  \n",
      "2      1975.0              6198.0         NaN          NaT  \n",
      "3      1625.0              9485.0         NaN          NaT  \n",
      "4      1354.0              9386.0         NaN          NaT  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge datasets\n",
    "\n",
    "# add to list\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "dataframes = [df_hr_final, df_sc, df_nt]\n",
    "\n",
    "# merge dataframes\n",
    "df_merged = pd.DataFrame()\n",
    "\n",
    "df_merged = reduce(lambda left, right: pd.merge(left, right, on=['participantId', 'startDate'], how='outer'), dataframes)\n",
    "\n",
    "# drop duplicates\n",
    "df_merged.drop_duplicates(subset=['participantId', 'startDate'], inplace=True)\n",
    "# drop timestamp\n",
    "#df_merged.drop(columns=['timestamp'], inplace=True)\n",
    "print (df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with other dataframes\n",
    "\n",
    "dataframes = [ df_onboarding, df_my_health, df_about_me, df_bmi, df_sq]\n",
    "\n",
    "for df in dataframes:\n",
    "    if 'timestamp' in df.columns:\n",
    "        df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "# merge dataframes\n",
    "df_surveys = pd.DataFrame()\n",
    "\n",
    "df_surveys = reduce(lambda left, right: pd.merge(left, right, on=['participantId'], how='outer', suffixes=('', '_y')), dataframes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 43.5 GiB for an array with shape (5837071630,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# merge dataframes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df_merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m---> 10\u001b[0m df_merged \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparticipantId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_y\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(left, right)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# merge dataframes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df_merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m---> 10\u001b[0m df_merged \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m left, right: \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparticipantId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_y\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, dataframes)\n",
      "File \u001b[1;32mc:\\Users\\aoife\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:162\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m    148\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    149\u001b[0m         left,\n\u001b[0;32m    150\u001b[0m         right,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aoife\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:809\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m--> 809\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[0;32m    812\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    813\u001b[0m )\n\u001b[0;32m    814\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n",
      "File \u001b[1;32mc:\\Users\\aoife\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1065\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     join_index, right_indexer, left_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[0;32m   1062\u001b[0m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[0;32m   1063\u001b[0m     )\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1065\u001b[0m     (left_indexer, right_indexer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1067\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index:\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\aoife\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1038\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_join_indexers\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp], npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp]]:\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"return the join indexers\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_join_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhow\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aoife\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1690\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[1;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[0;32m   1680\u001b[0m join_func \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m: libjoin\u001b[38;5;241m.\u001b[39minner_join,\n\u001b[0;32m   1682\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m: libjoin\u001b[38;5;241m.\u001b[39mleft_outer_join,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m: libjoin\u001b[38;5;241m.\u001b[39mfull_outer_join,\n\u001b[0;32m   1687\u001b[0m }[how]\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;66;03m# error: Cannot call function of unknown type\u001b[39;00m\n\u001b[1;32m-> 1690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjoin_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aoife\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\join.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.join.full_outer_join\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 43.5 GiB for an array with shape (5837071630,) and data type int64"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataframes = [df_merged, df_surveys]\n",
    "\n",
    "for df in dataframes:\n",
    "    if 'timestamp' in df.columns:\n",
    "        df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "# merge dataframes\n",
    "df_merged = pd.DataFrame()\n",
    "\n",
    "df_merged = reduce(lambda left, right: pd.merge(left, right, on=['participantId'], how='outer', suffixes=('', '_y')), dataframes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nap tracker, fill nan with 0\n",
    "\n",
    "df_merged['NapDuration'].fillna(0, inplace=True)\n",
    "\n",
    "# for menopause, fill nan with 3\n",
    "df_merged['menopause'].fillna(3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          participantId   startDate  morning_hr  afternoon_hr  \\\n",
      "0  06bc6ebb-a233-469f-8091-90256f656b1b  2016-03-13    1.173082      1.500000   \n",
      "1  2214f4fd-1ae0-4804-8663-b01c5f6d142c  2016-03-02    1.260992      1.333000   \n",
      "2  b2571643-4aec-492f-bc7f-6f23c7fe239a  2018-10-03    1.300000      1.263071   \n",
      "3  b2571643-4aec-492f-bc7f-6f23c7fe239a  2018-10-03    1.300000      1.263071   \n",
      "4  b2571643-4aec-492f-bc7f-6f23c7fe239a  2018-10-03    1.300000      1.263071   \n",
      "\n",
      "   evening_hr  night_hr  mean_hr_morning  max_hr_morning  min_hr_morning  \\\n",
      "0    1.173082  1.173082         1.159673        1.233000        0.867000   \n",
      "1    1.260992  1.260992         1.260992        1.260992        1.260992   \n",
      "2    1.263071  1.263071         1.285036        1.533000        1.133000   \n",
      "3    1.263071  1.263071         1.285036        1.533000        1.133000   \n",
      "4    1.263071  1.263071         1.285036        1.533000        1.133000   \n",
      "\n",
      "   mean_hr_afternoon  ...  timestamp_y  gender  age_years  startDate_y  \\\n",
      "0           1.185244  ...          NaT    Male       47.0   2016-03-13   \n",
      "1           1.261396  ...          NaT    Male       33.0   2016-03-02   \n",
      "2           1.241107  ...          NaT  Female       43.0   2018-10-03   \n",
      "3           1.241107  ...          NaT  Female       43.0   2018-10-03   \n",
      "4           1.241107  ...          NaT  Female       43.0   2018-10-03   \n",
      "\n",
      "   cancer  diabetes  stressed  menopause value  startDate_y  \n",
      "0     NaN       NaN       NaN        3.0   NaN          NaN  \n",
      "1     NaN       NaN       NaN        3.0   NaN          NaN  \n",
      "2     2.0       2.0       3.0        3.0   2.0   2018-10-05  \n",
      "3     2.0       2.0       3.0        3.0   3.0   2018-10-06  \n",
      "4     2.0       2.0       3.0        3.0   3.0   2018-10-07  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_merged.head())\n",
    "\n",
    "# drop duplicates\n",
    "df_merged.drop_duplicates(subset=['participantId', 'startDate'], inplace=True)\n",
    "\n",
    "# save to csv\n",
    "df_merged.to_csv(save_path + 'merged_data_before_dropna.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: before this, remove extra startdates\n",
    "\n",
    "\n",
    "# remove NaN\n",
    "df_merged = df_merged.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "\n",
    "df_merged.to_csv(save_path + 'initial_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# print number of participants in merged data\n",
    "print(len(df_merged['participantId'].unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
