{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Energy Burned\n",
    "Active energy burned and basal energy burned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load file into dataframe\n",
    "folder_path = 'c:/Users/aoife/Documents/Project/DataTables/'\n",
    "\n",
    "df_aeb = pd.read_csv(folder_path + 'active-energy-burned.csv', usecols=['participantId', 'timestamp', 'startTime', 'endTime', 'value'], parse_dates=['timestamp', 'startTime', 'endTime'])\n",
    "df_beb = pd.read_csv(folder_path + 'basal-energy-burned.csv', usecols=['participantId', 'timestamp', 'startTime', 'endTime', 'value'], parse_dates=['timestamp', 'startTime', 'endTime'])\n",
    "\n",
    "print(df_aeb.head())\n",
    "print('----------------\\n')\n",
    "print(df_beb.head())\n",
    "\n",
    "# Create a list of dataframes\n",
    "\n",
    "energy_dfs = [df_aeb, df_beb]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column which contains the difference between the start and end time\n",
    "\n",
    "for df in energy_dfs:\n",
    "    if 'startTime' and 'endTime' in df.columns:\n",
    "        df['duration'] = df['endTime'] - df['startTime']\n",
    "        \n",
    "        df['duration'] = df['duration'].dt.total_seconds()\n",
    "\n",
    "        \n",
    "        print(df.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the startTime to contain just the date\n",
    "\n",
    "for df in energy_dfs:\n",
    "    if 'startTime' in df.columns:\n",
    "        df['startTime'] = pd.to_datetime(df['startTime'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "        df['startDate'] = df['startTime'].dt.date\n",
    "\n",
    "        print(df.head())\n",
    "        print('----------------\\n')\n",
    "\n",
    "'''df_aeb['startTime'] = pd.to_datetime(df_aeb['startTime'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "df_aeb['startDate'] = df_aeb['startTime'].dt.date\n",
    "\n",
    "print(df_aeb.head())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total duration per day for each participant\n",
    "\n",
    "for df in energy_dfs:\n",
    "    if 'duration' in df.columns:\n",
    "        df['duration'] = df.groupby(['participantId', 'startDate'])['duration'].transform('sum')\n",
    "        df['value'] = df.groupby(['participantId', 'startDate'])['value'].transform('sum')\n",
    "\n",
    "        # Drop duplicates\n",
    "        df.drop_duplicates(subset=['participantId', 'startDate'], inplace=True)\n",
    "\n",
    "        print(df.head())\n",
    "        #print(df['value'].head())\n",
    "        print('----------------\\n')\n",
    "\n",
    "'''df_aeb['duration'] = df_aeb.groupby(['participantId', 'startDate'])['duration'].transform('sum')\n",
    "df_aeb['value'] = df_aeb.groupby(['participantId', 'startDate'])['value'].transform('sum')\n",
    "\n",
    "print(df_aeb['duration'].head())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total energy burned and total duration per day\n",
    "\n",
    "# Sort by participantId\n",
    "df_aeb.sort_values(by=['participantId'], inplace=True)\n",
    "df_beb.sort_values(by=['participantId'], inplace=True)\n",
    "\n",
    "# Rename value columns\n",
    "df_aeb.rename(columns={'value': 'active_energy'}, inplace=True)\n",
    "df_beb.rename(columns={'value': 'basal_energy'}, inplace=True)\n",
    "\n",
    "# Merge on participantId and startDate\n",
    "\n",
    "df_energy = pd.merge(df_aeb, df_beb, on=['participantId', 'startDate'])\n",
    "\n",
    "# Create a new column which contains the total energy burned per day\n",
    "\n",
    "df_energy['total_energy'] = df_energy['active_energy'] + df_energy['basal_energy']\n",
    "\n",
    "# Create a new column which contains the total duration per day\n",
    "\n",
    "df_energy['total_duration'] = df_energy['duration_x'] + df_energy['duration_y']\n",
    "\n",
    "# Rename duration columns\n",
    "df_energy.rename(columns={'duration_x': 'active_duration'}, inplace=True)\n",
    "df_energy.rename(columns={'duration_y': 'basal_duration'}, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "\n",
    "df_energy.drop(columns=['timestamp_x', 'startTime_x', 'endTime_x', 'timestamp_y', 'startTime_y', 'endTime_y'], inplace=True)\n",
    "\n",
    "print(df_energy.head())\n",
    "print('----------------\\n')\n",
    "print(df_energy['total_duration'].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMI\n",
    "\n",
    "Body Mass and Height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the groupby function to get the average height/body mass for each participant\n",
    "\n",
    "df_bm = pd.read_csv(folder_path + 'body-mass.csv', usecols=['participantId', 'timestamp', 'value'], parse_dates=['timestamp'])\n",
    "df_h = pd.read_csv(folder_path + 'height.csv', usecols=['participantId', 'timestamp', 'value'], parse_dates=['timestamp'])\n",
    "\n",
    "bmi_dfs = [df_bm, df_h]\n",
    "\n",
    "for df in bmi_dfs:\n",
    "    if 'value' in df.columns:\n",
    "\n",
    "        # Convert timestamp to datetime\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "        # Convert timestamp to date\n",
    "\n",
    "        # Get the average value for each participant\n",
    "        df['value'] = df.groupby(['participantId'])['value'].transform('mean')\n",
    "\n",
    "        # Drop duplicates\n",
    "        df.drop_duplicates(subset=['participantId'], inplace=True)\n",
    "\n",
    "        print(df.head())\n",
    "        print('----------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the BMI for each participant and create a new df\n",
    "\n",
    "# Sort by participantId\n",
    "df_bm.sort_values(by=['participantId'], inplace=True)\n",
    "df_h.sort_values(by=['participantId'], inplace=True)\n",
    "\n",
    "# Rename value columns\n",
    "df_bm.rename(columns={'value': 'bodyMass_kg'}, inplace=True)\n",
    "df_h.rename(columns={'value': 'height_m'}, inplace=True)\n",
    "\n",
    "# Merge on participantId\n",
    "\n",
    "df_bmi = pd.merge(df_bm, df_h, on='participantId')\n",
    "\n",
    "df_bmi['startDate'] = df_bmi['timestamp_x'].dt.date\n",
    "\n",
    "df_bmi['bmi'] = df_bmi['bodyMass_kg'] / (df_bmi['height_m'] ** 2)\n",
    "\n",
    "# drop unnecessary columns and duplicates\n",
    "df_bmi.drop(columns=['timestamp_x', 'timestamp_y'], inplace=True)\n",
    "df_bmi.drop_duplicates(subset=['participantId'], inplace=True)\n",
    "\n",
    "print(df_bmi.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_hr = pd.read_csv(folder_path + 'heart-rate.csv', usecols=['participantId', 'timestamp', 'startTime', 'endTime', 'value'], parse_dates=['timestamp', 'startTime', 'endTime'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert startTime to datetime and extract date\n",
    "df_hr['startTime'] = pd.to_datetime(df_hr['startTime'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "df_hr['endTime'] = pd.to_datetime(df_hr['endTime'], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "df_hr['startDate'] = df_hr['startTime'].dt.date\n",
    "print(df_hr.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Remove rows where participantId, startTime and endTime are duplicated\n",
    "\n",
    "df_hr.drop_duplicates(subset=['participantId', 'startTime', 'endTime'], inplace=True)\n",
    "\n",
    "print(df_hr.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows per person per day\n",
    "\n",
    "df_hr['hr_duration_(s)'] = df_hr.groupby(['participantId', 'startDate'])['value'].transform('count')\n",
    "df_hr['hr_duration_(s)'] = df_hr['hr_duration_(s)'].astype(float)\n",
    "\n",
    "# get the maximum heart rate per day\n",
    "df_hr['max_hr'] = df_hr.groupby(['participantId', 'startDate'])['value'].transform('max')\n",
    "\n",
    "# get the minimum heart rate per day\n",
    "df_hr['min_hr'] = df_hr.groupby(['participantId', 'startDate'])['value'].transform('min')\n",
    "\n",
    "# get the average heart rate per day\n",
    "df_hr['value'] = df_hr.groupby(['participantId', 'startDate'])['value'].transform('mean')\n",
    "\n",
    "\n",
    "\n",
    "# Drop duplicates\n",
    "df_hr.drop_duplicates(subset=['participantId', 'startDate'], inplace=True)\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_hr.drop(columns=['timestamp', 'startTime', 'endTime'], inplace=True)\n",
    "\n",
    "# Rename value column\n",
    "df_hr.rename(columns={'value': 'mean_hr/s'}, inplace=True)\n",
    "\n",
    "print(df_hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nap Tracker\n",
    "\n",
    "Total duration\n",
    "\n",
    "Average nap quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nt = pd.read_csv(folder_path + 'nap-tracker.csv', usecols=['participantId', 'timestamp', 'NapDuration', 'NapQuality'], parse_dates=['timestamp'])\n",
    "\n",
    "# calculate total nap duration per day for each participant\n",
    "\n",
    "df_nt['timestamp'] = pd.to_datetime(df_nt['timestamp'], utc=True)\n",
    "df_nt['startDate'] = df_nt['timestamp'].dt.date\n",
    "\n",
    "# Remove rows where startDate, participantId and NapDuration are duplicated\n",
    "df_nt.drop_duplicates(subset=['participantId', 'startDate', 'NapDuration'], inplace=True)\n",
    "\n",
    "df_nt['totalNapTime_(s)'] = df_nt.groupby(['participantId', 'startDate'])['NapDuration'].transform('sum')\n",
    "\n",
    "# Get the average nap quality per day for each participant\n",
    "\n",
    "df_nt['avgNapQuality'] = df_nt.groupby(['participantId', 'startDate'])['NapQuality'].transform('mean')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_nt.drop(columns=['timestamp', 'NapDuration', 'NapQuality'], inplace=True)\n",
    "\n",
    "# If NaN is present in the avgNapQuality or totalNapTime column, fill with 0\n",
    "df_nt['avgNapQuality'] = df_nt['avgNapQuality'].fillna(0)\n",
    "df_nt['totalNapTime_(s)'] = df_nt['totalNapTime_(s)'].fillna(0)\n",
    "\n",
    "print(df_nt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc = pd.read_csv(folder_path + 'step-count.csv', usecols=['participantId', 'startTime', 'endTime', 'value'], parse_dates=['startTime', 'endTime'])\n",
    "\n",
    "# get total step count per day for each participant\n",
    "\n",
    "df_sc['startTime'] = pd.to_datetime(df_sc['startTime'], utc=True)\n",
    "df_sc['endTime'] = pd.to_datetime(df_sc['endTime'], utc=True)\n",
    "df_sc['startDate'] = df_sc['startTime'].dt.date\n",
    "\n",
    "df_sc['duration'] = df_sc['endTime'] - df_sc['startTime']\n",
    "df_sc['duration'] = df_sc['duration'].dt.total_seconds()\n",
    "\n",
    "df_sc['totalSteps'] = df_sc.groupby(['participantId', 'startDate'])['value'].transform('sum')\n",
    "df_sc['totalSteps'] = df_sc['totalSteps'].astype(float)\n",
    "\n",
    "# Get the total duration per day for each participant\n",
    "df_sc['stepsTotalDuration'] = df_sc.groupby(['participantId', 'startDate'])['duration'].transform('sum')\n",
    "\n",
    "\n",
    "# Get rid of duplicates\n",
    "\n",
    "df_sc.drop_duplicates(subset=['participantId', 'startDate'], inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_sc.drop(columns=['value', 'duration', 'startTime', 'endTime'], inplace=True)\n",
    "\n",
    "print(df_sc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Walking/Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "\n",
    "df_dwr = pd.read_csv(folder_path + 'distance-walking-running.csv', usecols=['participantId', 'timestamp', 'startTime', 'endTime', 'value'], parse_dates=['timestamp'])\n",
    "\n",
    "# Convert timestamp to datetime and extract date\n",
    "\n",
    "df_dwr['startTime'] = pd.to_datetime(df_dwr['startTime'], utc=True)\n",
    "df_dwr['endTime'] = pd.to_datetime(df_dwr['endTime'], utc=True)\n",
    "df_dwr['timestamp'] = pd.to_datetime(df_dwr['timestamp'], utc=True)\n",
    "\n",
    "df_dwr['startDate'] = df_dwr['startTime'].dt.date\n",
    "\n",
    "# Get the total distance walked/ran per day for each participant\n",
    "df_dwr['totalDistance'] = df_dwr.groupby(['participantId', 'startDate'])['value'].transform('sum')\n",
    "\n",
    "# Get the duration\n",
    "df_dwr['duration'] = df_dwr['endTime'] - df_dwr['startTime']\n",
    "df_dwr['duration'] = df_dwr['duration'].dt.total_seconds()\n",
    "\n",
    "df_dwr['dwrTotalDuration'] = df_dwr.groupby(['participantId', 'startDate'])['duration'].transform('sum')\n",
    "\n",
    "# Drop duplicates\n",
    "df_dwr.drop_duplicates(subset=['participantId', 'startDate'], inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_dwr.drop(columns=['timestamp', 'startTime', 'endTime', 'duration', 'value'], inplace=True)\n",
    "\n",
    "\n",
    "print(df_dwr.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep Quality\n",
    "\n",
    "For sleep quality, I will change the timestamp to just include the date, and ensure there are no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sq = pd.read_csv(folder_path + 'sleep-quality-checker.csv', usecols=['participantId', 'timestamp', 'value'], parse_dates=['timestamp'])\n",
    "\n",
    "df_sq['timestamp'] = pd.to_datetime(df_sq['timestamp'], utc=True)\n",
    "\n",
    "df_sq['startDate'] = df_sq['timestamp'].dt.date\n",
    "\n",
    "# drop duplicates\n",
    "df_sq.drop_duplicates(subset=['participantId', 'startDate'], inplace=True)\n",
    "\n",
    "# drop unnecessary columns\n",
    "df_sq.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "df_sq.rename(columns={'value': 'ssq_score'}, inplace=True)\n",
    "\n",
    "print(df_sq.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Participants\n",
    "\n",
    "The amount of rows in these dataframes are still too large, so I will randomly sample a portion of the participants.\n",
    "\n",
    "I will first of all find the participants which have data in each dataframe, on the same days (except BMI).\n",
    "\n",
    "Then I use a probability to keep a certain portion of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_sampling():\n",
    "    # Create a list of DataFrames\n",
    "    dfs_list = [df_bmi, df_energy, df_hr, df_nt, df_sc, df_dwr, df_sq]\t\n",
    "\n",
    "\n",
    "    # Create a list of all participants\n",
    "    participants = set(dfs_list[0]['participantId'].unique())\n",
    "\n",
    "    # Check that the common participants have a value for sleep quality score\n",
    "    participants = participants.intersection(set(df_sq[~df_sq['ssq_score'].isnull()]['participantId'].unique()))\n",
    "\n",
    "    #print(participants)\n",
    "\n",
    "    # Probability of keeping a participant\n",
    "    keep_probability = 0.7\n",
    "\n",
    "    # Determine which participants to keep based on the probability\n",
    "    participants_to_keep = np.random.choice(list(participants), size=int(len(participants) * keep_probability), replace=False)\n",
    "\n",
    "    # Filter each original DataFrame to keep only the common participants\n",
    "    filtered_dfs = [df[df['participantId'].isin(participants_to_keep)] for df in dfs_list]\n",
    "\n",
    "    # Checking values\n",
    "    for i, df in enumerate(filtered_dfs):\n",
    "\n",
    "        #print(f'Columns of filtered_df[{i}]: {df.columns}')\n",
    "\n",
    "        unique_participants = df['participantId'].unique()\n",
    "        '''print(f\"Unique participants in DataFrame {i}:\")\n",
    "        print(unique_participants)\n",
    "        print(f\"Number of unique participants: {len(unique_participants)}\")\n",
    "        print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "        print(f\"DataFrame {i}:\")\n",
    "        #print(df)\n",
    "        print(f\"Number of rows: {len(df)}\")\n",
    "        print(\"\\n\" + \"-\"*30 + \"\\n\")'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative to Randomly Sampling\n",
    "\n",
    "For this alternative, rather than randomly sampling from the pool of participants that are common across all dataframes, I will first sort participants alphabetically in each dataframe, and add them to a list. \n",
    "\n",
    "Then, I will iterate through the first $i$ participants in that list, and merge the data present for those.\n",
    "\n",
    "I will then get the next $i$ participants and perform a merge, and continute until there are $i$ or less participants - then I will merge those.\n",
    "\n",
    "With each of these subsets, I will remove rows where there is missing data. \n",
    "\n",
    "Then, I will merge all of the complete subsets together, RAM permitting.\n",
    "    If the data is still to large to perform a merge, I will randomly sample participants from the complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge function\n",
    "\n",
    "def merge_dfs(dfs_list, participants):\n",
    "    merged_df = None\n",
    "    for df in dfs_list:\n",
    "        # Filter dataframe for participants we currently work with\n",
    "        curr_df = df[df['participantId'].isin(participants)]\n",
    "        if merged_df is None:\n",
    "            merged_df = curr_df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, curr_df, on=['participantId', 'startDate'], how='outer', suffixes=('', '_y'))\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method of gathering a smaller dataset\n",
    "\n",
    "def get_separate_tables(num_participants):\n",
    "    \n",
    "    # Create a list of DataFrames\n",
    "    dfs_list = [df_bmi, df_hr, df_sc, df_dwr, df_sq]\t\n",
    "\n",
    "    # Create a list of all participants\n",
    "    participants = set(dfs_list[0]['participantId'].unique())\n",
    "\n",
    "    # Sort the list of participants alphabetically\n",
    "    participants = sorted(list(participants))\n",
    "\n",
    "    result_dfs = []\n",
    "\n",
    "    while participants:\n",
    "        # Get the next group of participants\n",
    "        next_group = participants[:num_participants]\n",
    "\n",
    "        # Merge the DataFrames for the next group of participants\n",
    "        merge_df = merge_dfs(dfs_list, next_group)\n",
    "\n",
    "        print(merge_df.head())\n",
    "\n",
    "        # Add the merged DataFrame to the list of results\n",
    "        result_dfs.append(merge_df)\n",
    "\n",
    "        # Remove the participants that were just used\n",
    "        participants = participants[num_participants:]\n",
    "\n",
    "    return result_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call get_separate_tables() and check the resulting dfs\n",
    "\n",
    "subset_dfs = get_separate_tables(20)\n",
    "\n",
    "for i, df in enumerate(subset_dfs):\n",
    "    print(f\"DataFrame {i}:\")\n",
    "    print(df.head())\n",
    "    print(f\"Number of rows: {len(df)}\")\n",
    "    print(\"\\n\" + \"-\"*30 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each unique participant, check if they have a value for weight, height, and bmi. If so, copy values to each row for that participant\n",
    "\n",
    "for df in subset_dfs:\n",
    "\n",
    "    # Loop through participants\n",
    "    for participant in df['participantId'].unique():\n",
    "        # Find the index of the row containing non NaN values for weight, height, and bmi\n",
    "        non_nan_index = df[(df['participantId'] == participant) & (~df['bodyMass_kg'].isnull()) & (~df['height_m'].isnull()) & (~df['bmi'].isnull())].index\n",
    "\n",
    "        # If there is a row with non NaN values, copy the values to all rows for that participant\n",
    "        if len(non_nan_index) > 0:\n",
    "            non_nan_index = non_nan_index[0]\n",
    "            non_nan_row = df.loc[non_nan_index]\n",
    "            df.loc[df['participantId'] == participant, 'bodyMass_kg'] = non_nan_row['bodyMass_kg']\n",
    "            df.loc[df['participantId'] == participant, 'height_m'] = non_nan_row['height_m']\n",
    "            df.loc[df['participantId'] == participant, 'bmi'] = non_nan_row['bmi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Read in data from surveys\n",
    "# For each unique participnat in subset_dfs, check if they have answered both surveys. If so, copy the values to each row for that participant\n",
    "# Note see above code\n",
    "\n",
    "folder_path = 'c:/Users/aoife/Documents/Project/DataTables/'\n",
    "\n",
    "\n",
    "df_about_me = pd.read_csv(folder_path + 'about-me.csv', usecols=['participantId', 'timestamp', 'alcohol', 'basic_expenses', 'caffeine', 'daily_activities', 'daily_activities', 'daily_smoking', 'education', 'flexible_work_hours', 'gender', 'good_life', 'hispanic', 'income', 'marital', 'race', 'smoking_status', 'menopause', 'recent_births', 'current_pregnant', 'work_schedule'], parse_dates=['timestamp'])\n",
    "\n",
    "df_sleep_habits = pd.read_csv(folder_path + 'sleep-habits.csv', usecols=['participantId', 'timestamp', 'alarm_dependency', 'driving_sleepy', 'falling_asleep', 'morning_person', 'nap_duration', 'sleep_lost', 'sleep_needed', 'sleep_partner', 'sleep_time_workday', 'sleep_time_weekend', 'wake_up_choices', 'wake_ups', 'weekly_naps', 'what_wakes_you'], parse_dates=['timestamp'])\n",
    "\n",
    "df_sleep_assessment = pd.read_csv(folder_path + 'sleep-assessment.csv', usecols=['participantId', 'timestamp', 'alcohol', 'concentrating_problem_one', 'concentrating_problem_two', 'discomfort_in_sleep', 'exercise', 'fatigue_limit', 'feel_tired_frequency', 'felt_alert', 'had_problem', 'hard_times', 'medication_by_doctor', 'poor_sleep_problems', 'sleep_aids', 'sleep_problem', 'think_clearly', 'tired_easily', 'told_by_doctor', 'told_by_doctor_specify', 'told_to_doctor'], parse_dates=['timestamp'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the alcolhol column in about_me and sleep_assessment to be more detailed\n",
    "\n",
    "df_about_me.rename(columns={'alcohol': 'alcohol_consumption'}, inplace=True)\n",
    "df_sleep_assessment.rename(columns={'alcohol': 'alcohol_sleep_help'}, inplace=True)\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df_about_me['timestamp'] = pd.to_datetime(df_about_me['timestamp'], utc=True)\n",
    "df_sleep_habits['timestamp'] = pd.to_datetime(df_sleep_habits['timestamp'], utc=True)\n",
    "df_sleep_assessment['timestamp'] = pd.to_datetime(df_sleep_assessment['timestamp'], utc=True)\n",
    "\n",
    "# Change the timestamp to only contain the date\n",
    "df_about_me['date'] = df_about_me['timestamp'].dt.date\n",
    "df_sleep_habits['date'] = df_sleep_habits['timestamp'].dt.date\n",
    "df_sleep_assessment['date'] = df_sleep_assessment['timestamp'].dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge survey dfs\n",
    "\n",
    "surveys_list = [df_about_me, df_sleep_habits, df_sleep_assessment]\n",
    "\n",
    "# Merge the DataFrames\n",
    "df_surveys = pd.merge(surveys_list[0], surveys_list[1], on=['participantId', 'date'], how='outer', suffixes=('', '_y'))\n",
    "#df_surveys = pd.merge(df_surveys, surveys_list[2], on=['participantId', 'date'], how='outer', suffixes=('', '_y'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df_surveys.drop_duplicates(subset=['participantId'], inplace=True)\n",
    "\n",
    "# For the 'menopause' column, replace NaN with 3\n",
    "df_surveys['menopause'] = df_surveys['menopause'].fillna(3)\n",
    "\n",
    "# For the recent_births column, replace NaN with 4\n",
    "df_surveys['recent_births'] = df_surveys['recent_births'].fillna(4)\n",
    "\n",
    "# Replace current_pregnant NaN with 0\n",
    "df_surveys['current_pregnant'] = df_surveys['current_pregnant'].fillna(0)\n",
    "\n",
    "# replace driving_sleepy NaN with 6\n",
    "df_surveys['driving_sleepy'] = df_surveys['driving_sleepy'].fillna(6)\n",
    "\n",
    "# replace falling_asleep NaN with 0\n",
    "df_surveys['falling_asleep'] = df_surveys['falling_asleep'].fillna(0)\n",
    "\n",
    "# replace morning_person NaN with 3\n",
    "df_surveys['morning_person'] = df_surveys['morning_person'].fillna(3)\n",
    "\n",
    "# replace nap_duration NaN with \n",
    "df_surveys['nap_duration'] = df_surveys['nap_duration'].fillna(6)\n",
    "\n",
    "# replace sleep_lost NaN with 0\n",
    "df_surveys['sleep_lost'] = df_surveys['sleep_lost'].fillna(0)\n",
    "\n",
    "# replace what_wakes_you NaN with 13\n",
    "df_surveys['what_wakes_you'] = df_surveys['what_wakes_you'].fillna(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "\n",
    "df_surveys.drop(columns=['timestamp', 'date', 'timestamp_y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save to csv\n",
    "df_surveys.to_csv(folder_path + 'surveys.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to CSV's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered DataFrames to new CSV files\n",
    "\n",
    "save_path = 'C:\\\\Users\\\\aoife\\Documents\\\\Project\\\\filtered_data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows in each subset_df where there are NaN values\n",
    "\n",
    "total_rows = 0\n",
    "\n",
    "for i, df in enumerate(subset_dfs):\n",
    "    subset_dfs[i] = subset_dfs[i].dropna()\n",
    "    \n",
    "    print(subset_dfs[i].head())\n",
    "    # print the number of rows in the cleaned dataframe\n",
    "    print(f\"Number of rows in cleaned DataFrame {i}: {len(subset_dfs[i])}\")\n",
    "\n",
    "    #print the total rows\n",
    "    total_rows += len(subset_dfs[i])\n",
    "\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate Activity dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DataFrames in subset_dfs into a single DataFrame\n",
    "\n",
    "concatenated_df = pd.concat(subset_dfs, ignore_index=True)\n",
    "\n",
    "print(concatenated_df.head())\n",
    "\n",
    "# print total number of rows in concatenated_df\n",
    "print(f\"Total number of rows in concatenated_df: {len(concatenated_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged DataFrame to a new CSV file\n",
    "\n",
    "concatenated_df.to_csv(save_path + 'concatenated_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Surveys with Concatenated Activity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the surveys DataFrame with the concatenated DataFrame\n",
    "\n",
    "activity_and_survey_df = pd.merge(concatenated_df, df_surveys, on=['participantId'], how='outer', suffixes=('', '_y'))\n",
    "\n",
    "print(activity_and_survey_df.head())\n",
    "\n",
    "# Print num rows\n",
    "print(f\"Number of rows in activity_and_survey_df before NaN removed: {len(activity_and_survey_df)}\")\n",
    "\n",
    "# Remove rows that have NaN values\n",
    "activity_and_survey_df = activity_and_survey_df.dropna()\n",
    "\n",
    "print(f\"Number of rows in activity_and_survey_df after NaN removed: {len(activity_and_survey_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactor Columns\n",
    "\n",
    "The columns 'hispanic', 'race', 'sleep_partner', and 'what_wakes_you' have multiple values. I need to refactor these columns to only include 1 value per cell.\n",
    "\n",
    "For the 'hispanic' and 'race' columns, I will replace any multiple answers with '6', which in the survey indicated multiple races/multiple hispanic ethnicities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_multiple_race(row):\n",
    "    if ',' in row:\n",
    "        return 6\n",
    "    return row\n",
    "\n",
    "activity_and_survey_df['hispanic'] = activity_and_survey_df['hispanic'].apply(join_multiple_race)\n",
    "activity_and_survey_df['race'] = activity_and_survey_df['race'].apply(join_multiple_race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sleep_partner column I will add an option 'multiple' = 6, to indicate that this participant sleeps with multiple of the options in their room (e.g., they sleep with their pets and their significant other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_multiple_sleep_partner(row):\n",
    "    if ',' in row:\n",
    "        return 6\n",
    "    return row\n",
    "\n",
    "activity_and_survey_df['sleep_partner'] = activity_and_survey_df['sleep_partner'].apply(join_multiple_sleep_partner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "\n",
    "activity_and_survey_df.to_csv(save_path + 'activity_and_survey_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I check what reasons each participant provided for waking up at night.\n",
    "\n",
    "I add them to new rows where 1 indicates they suffer from this reason, 0 indicates they do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns for wakeup reasons\n",
    "\n",
    "activity_and_survey_df['noise_light'] = 0\n",
    "activity_and_survey_df['stress_thinking'] = 0\n",
    "activity_and_survey_df['other_person'] = 0\n",
    "activity_and_survey_df['pain_discomfort'] = 0\n",
    "activity_and_survey_df['nightmares'] = 0\n",
    "activity_and_survey_df['bathroom_urges'] = 0\n",
    "activity_and_survey_df['other_reasons'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_wakeup_reason(row, number):\n",
    "    if number == 1:\n",
    "        activity_and_survey_df.loc[row.name, 'noise_light'] = 1\n",
    "    elif number == 2:\n",
    "        activity_and_survey_df.loc[row.name, 'stress_thinking'] = 1\n",
    "    elif number == 3:\n",
    "        activity_and_survey_df.loc[row.name, 'other_person'] = 1\n",
    "    elif number == 4:\n",
    "        activity_and_survey_df.loc[row.name, 'pain_discomfort'] = 1\n",
    "    elif number == 5:\n",
    "        activity_and_survey_df.loc[row.name, 'nightmares'] = 1\n",
    "    elif number == 6:\n",
    "        activity_and_survey_df.loc[row.name, 'bathroom_urges'] = 1\n",
    "    else:\n",
    "        activity_and_survey_df.loc[row.name, 'other_reasons'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in activity_and_survey_df.iterrows():\n",
    "    if ',' in row['what_wakes_you']:\n",
    "        nums = map(int, row['what_wakes_you'].split(','))\n",
    "        for number in nums:\n",
    "            check_wakeup_reason(row, number)\n",
    "    else:\n",
    "        check_wakeup_reason(row, int(row['what_wakes_you']))\n",
    "\n",
    "# Remove what_wakes_you\n",
    "activity_and_survey_df.drop(columns=['what_wakes_you'], inplace=True)\n",
    "\n",
    "print(activity_and_survey_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "\n",
    "activity_and_survey_df.to_csv(save_path + 'activity_and_survey_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
